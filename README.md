# Reading-list-in-2019

## Knowledge Distillation:
- **Basic work**: "Distilling the Knowledge in a Neural Network", in NIPS, 2014.
- **Mimic Feature Maps**: "Fitnets: Hints for thin deep nets", in ICLR, 2014.
- **Mimic Attention Maps**: "Paying more attention to attention: Improving the performance of CNN via attention transfer", in ICLR, 2017.
- **Mimic Weight Matrix**: "A gift from knowledge distillation: Fast Optimization, Network Minimization and Transfer Learning", in CVPR, 2017.
- **Mimic Feature Dist.**: "Like What You Like: Knowledge Distill via Neuron Selectivity Transfer", in CVPR, 2017.
- **Mimic by Adversary**: "Training Shallow and Thin Networks for Acceleration via Knowledge Distillation with Conditional Adversarial Networks", 2017.
- **Multi teachers**: "Amalgamating Knowledge towards Comprehensive Classification", in AAAI, 2019.
- **DML**: "Deep Mutual Learning", in CVPR, 2018.


## Learning with Noisy Labels:
- **Co-teach**: "Co-teaching: Robust Training of Deep Neural Networks with Extremely Noisy Labels", in NIPS, 2018.
- **Mutual Attention**: "A Two-Stream Mutual Attention Network for Semi-supervised Biomedical Segmentation with Noisy Labels", in AAAI, 2019.


## Domain Adaptation:
- **Basic work**: "Domain-Adversarial Training of Neural Networks", in Journal of Machine Learning Research, 2016.
- **Attention based**: "Transferable Attention for Domain Adaptation", in AAAI, 2019.
- **Structured KD**: "Structured Knowledge Distillation for Semantic Segmentation", in CVPR, 2019.


## Weakly Supervised:
- **Cls. & Seg.**: "Weakly-Supervised Simultaneous Evidence Identification and Segmentation for Automated Glaucoma Diagnosis", in AAAI, 2019. (写的一般，没太看懂)．
